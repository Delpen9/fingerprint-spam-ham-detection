{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f38c9a-d770-4868-8b98-a2da3180d5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mermaid-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98555c8f-14ec-40d1-9cea-93a846930320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mermaid as md\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee6507a9-03b7-43e5-b23b-7e19c3f8cf85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_mermaid_diagram(diagram: str) -> None:\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <script type=\"module\">\n",
    "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n",
    "            mermaid.initialize({{ startOnLoad: true }});\n",
    "\n",
    "            document.addEventListener(\"DOMContentLoaded\", () => {{\n",
    "                setTimeout(() => generateHighQualityJPEG(), 1500); // Wait for Mermaid rendering\n",
    "            }});\n",
    "\n",
    "            function generateHighQualityJPEG() {{\n",
    "                const svgElement = document.querySelector('.mermaid svg');\n",
    "                if (!svgElement) return;\n",
    "\n",
    "                const serializer = new XMLSerializer();\n",
    "                const svgString = serializer.serializeToString(svgElement);\n",
    "                \n",
    "                // Convert SVG string to Data URL\n",
    "                const svgDataUrl = \"data:image/svg+xml;charset=utf-8,\" + encodeURIComponent(svgString);\n",
    "                \n",
    "                const img = new Image();\n",
    "                img.onload = function() {{\n",
    "                    const scaleFactor = 3; // High-resolution scaling factor\n",
    "                    const canvas = document.createElement(\"canvas\");\n",
    "                    const width = svgElement.clientWidth * scaleFactor;\n",
    "                    const height = svgElement.clientHeight * scaleFactor;\n",
    "                    canvas.width = width;\n",
    "                    canvas.height = height;\n",
    "                    \n",
    "                    const context = canvas.getContext(\"2d\");\n",
    "                    context.fillStyle = \"white\"; // Ensure background is white\n",
    "                    context.fillRect(0, 0, canvas.width, canvas.height);\n",
    "                    context.drawImage(img, 0, 0, width, height); // Scale image to high resolution\n",
    "\n",
    "                    // Convert to high-quality JPEG\n",
    "                    const jpegUrl = canvas.toDataURL(\"image/jpeg\", 0.95); // 95% quality\n",
    "                    const downloadLink = document.getElementById(\"download-jpeg\");\n",
    "                    downloadLink.href = jpegUrl;\n",
    "                    downloadLink.download = \"high_quality_diagram.jpeg\";\n",
    "                    downloadLink.style.display = \"inline-block\";\n",
    "                }};\n",
    "                img.src = svgDataUrl;\n",
    "            }}\n",
    "        </script>\n",
    "        <style>\n",
    "            body {{\n",
    "                background-color: white;\n",
    "                text-align: center;\n",
    "                font-family: 'Arial', sans-serif;\n",
    "                margin: 0;\n",
    "                padding: 20px;\n",
    "            }}\n",
    "            .diagram-container {{\n",
    "                display: flex;\n",
    "                justify-content: center;\n",
    "                align-items: center;\n",
    "                padding: 20px;\n",
    "            }}\n",
    "            #download-jpeg {{\n",
    "                display: none;\n",
    "                margin-top: 15px;\n",
    "                padding: 12px 18px;\n",
    "                font-size: 16px;\n",
    "                font-weight: bold;\n",
    "                background-color: #4CAF50;\n",
    "                color: white;\n",
    "                border: none;\n",
    "                cursor: pointer;\n",
    "                text-decoration: none;\n",
    "                border-radius: 6px;\n",
    "                transition: background-color 0.3s ease;\n",
    "            }}\n",
    "            #download-jpeg:hover {{\n",
    "                background-color: #45a049;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2>Mermaid Diagram Viewer</h2>\n",
    "        <div class=\"diagram-container\">\n",
    "            <div class=\"mermaid\">{diagram}</div>\n",
    "        </div>\n",
    "        <a id=\"download-jpeg\">Download High-Quality JPEG</a>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    from IPython.display import HTML, display\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc5a48f-aa39-4eaa-9a5c-443ef43d3c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_mmd_file(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb6e2a5e-a0a7-4f6b-8be9-ff87e3a915ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Your flowchart outlines a well-structured pipeline for building and continuously improving an audio-call classification system using OpenAI Whisper (or any modern speech recognition model). The steps—ranging from raw data capture, transcription, manual labeling, model fine-tuning, classification, confidence checking, and an active learning loop—are aligned with best practices in machine learning pipelines. Here are some strengths and considerations:\n",
    "\n",
    "  1. **Strengths**\n",
    "    •\tActive Learning Loop: Sending low-confidence predictions back for human labeling ensures the model is constantly improving on “hard” or ambiguous cases.\n",
    "    •\tIterative Retraining: Continuously retraining on newly labeled data helps your model stay up to date and reduces drift.\n",
    "    •\tFingerprint Database: Storing “fingerprints” (or acoustic signatures) for future filtering can be a powerful way to avoid reprocessing calls that match previously known patterns.\n",
    "\n",
    "  2. **Potential Enhancements**\n",
    "    •\tConfidence Calibration: Ensure the model’s confidence scores are well-calibrated (e.g., using temperature scaling or other calibration methods) so the threshold for “high confidence” is meaningful.\n",
    "    •\tData Privacy & Compliance: Depending on the jurisdiction, capturing and storing call data may require special handling (GDPR, HIPAA, etc.).\n",
    "    •\tData Augmentation: Consider augmenting audio data (e.g., adding noise, varying pitch, speed) to improve model robustness.\n",
    "    •\tEdge Cases: Ensure you have a plan for out-of-vocabulary terms, strong accents, or poor audio quality.\n",
    "\n",
    "Below are some references and resources to help you dive deeper into each section of the plan.\n",
    "\n",
    "  1. **Capturing and Managing Audio Call Data**\n",
    "    •\tAudio Data Acquisition & Telephony\n",
    "    •\tTwilio Docs: Twilio Voice covers how to capture calls programmatically.\n",
    "    •\tAsterisk PBX: Asterisk is an open-source framework for building communications applications and can be used to capture call audio.\n",
    "    •\tAudio Processing Basics\n",
    "    •\tLibrosa: Librosa GitHub – Python library for audio and music analysis.\n",
    "    •\tKaldi: Kaldi – a toolkit for speech recognition that also provides tools for data handling and feature extraction.\n",
    "\n",
    "  2. **Transcribing Audio with OpenAI Whisper**\n",
    "    •\tOpenAI Whisper\n",
    "    •\tOfficial GitHub Repository: openai/whisper – contains usage instructions, model details, and examples.\n",
    "    •\tWhisper Fine-Tuning Tutorial: While Whisper is often used as-is for transcription, there are community-driven tutorials on partial fine-tuning for specific domains (e.g., call center audio, medical transcription).\n",
    "    •\tSpeech-to-Text Concepts\n",
    "    •\tCMU Sphinx: CMU Sphinx – older, but well-documented speech recognition toolkit to understand basics of ASR (Automatic Speech Recognition).\n",
    "    •\tHugging Face Transformers: Speech Recognition – for alternative ASR models and code examples.\n",
    "\n",
    "  3. **Manual Labeling and Data Annotation**\n",
    "    •\tBest Practices for Labeling\n",
    "    •\tBurr Settles (2010). “Active Learning Literature Survey.” PDF – while it focuses on active learning, it also covers best practices in labeling and iterative model improvement.\n",
    "    •\tAmazon SageMaker Ground Truth: Ground Truth – a managed service for building highly accurate training datasets.\n",
    "    •\tLabelbox: Labelbox – platform for labeling data, including audio.\n",
    "    •\tGuidelines & Tools\n",
    "    •\tVGG Annotator or Audacity: Basic tools that can help slice audio and annotate transcripts.\n",
    "    •\tSnorkel: Snorkel AI – focuses on programmatic labeling and weak supervision, which can speed up manual labeling.\n",
    "\n",
    "  4. **Fine-Tuning Whisper with a Classification Head**\n",
    "    •\tSpeech Classification / Fine-Tuning\n",
    "    •\tHugging Face Tutorials: Fine-Tuning Wav2Vec2 for Speech Classification – While this tutorial is for Wav2Vec2, the general approach is similar for Whisper or other ASR models.\n",
    "    •\tTransfer Learning in NLP: CS224N (Stanford) – covers fundamental techniques for transfer learning in language models, which can apply to speech.\n",
    "    •\tModel Architecture & Adding Classification Heads\n",
    "    •\tPyTorch Tutorials: Custom Head in PyTorch – for adding a classification layer on top of a pre-trained encoder.\n",
    "    •\tOpenAI Whisper + Classification Example (Community Project): Some developers have shared experiments on GitHub or Hugging Face forums for multi-task training (transcription + classification).\n",
    "\n",
    "  5. **Classification & Confidence Scoring**\n",
    "    •\tConfidence/Uncertainty Estimation\n",
    "    •\tCalibration Methods: “On Calibration of Modern Neural Networks” by Guo et al. (2017). ArXiv\n",
    "    •\tBayesian Methods: Yarin Gal’s blog. Focuses on Bayesian deep learning, which can help model uncertainty.\n",
    "    •\tReal-Time Inference\n",
    "    •\tModel Deployment: TensorRT, ONNX Runtime – for optimizing inference speed, which may be crucial in live call scenarios.\n",
    "\n",
    "  6. **Active Learning and Iterative Retraining**\n",
    "    •\tCore Active Learning Literature\n",
    "    •\tBurr Settles (2010). “Active Learning Literature Survey.” – the go-to reference.\n",
    "    •\tActive Learning Blog Posts & Tutorials: Hugging Face Active Learning Blog sometimes has articles on iterative dataset labeling.\n",
    "    •\tPractical Implementation\n",
    "    •\tModAL: ModAL – a Python framework for active learning.\n",
    "    •\tHuman-in-the-Loop ML: Fast.ai Forums – many discussions on building iterative ML systems with manual labeling in the loop.\n",
    "\n",
    "  7. **Fingerprinting and Future Filtering**\n",
    "    •\tAcoustic Fingerprinting\n",
    "    •\tShazam-Like Approaches: Music/Audio Fingerprinting Resources – although aimed at music, the core concepts apply to call audio.\n",
    "    •\tImage/Audio Hashing: Techniques like MinHash, SimHash, or spectral hashing can be adapted for audio.\n",
    "    •\tCall Spam/Filtering\n",
    "    •\tSTIR/SHAKEN (Telecom): FCC Resources – a protocol suite for authenticating calls, not exactly “fingerprinting,” but relevant to call security/spam.\n",
    "    •\tML-based Spam Detection: “Robocall Detection using Machine Learning” – various IEEE papers discuss spam detection methods.\n",
    "\n",
    "  8. **Retraining with Low-Confidence Samples**\n",
    "    •\tIterative Model Improvement\n",
    "    •\tAndrew Ng’s “Data-Centric AI” concept: Landing AI Blog – emphasizes the importance of iterative improvements on challenging data.\n",
    "    •\tData Augmentation for Speech: SpecAugment paper – a technique to improve robustness by distorting spectrograms.\n",
    "\n",
    "**Putting It All Together**\n",
    "- Data Flow: You have a clear data ingestion pipeline (raw calls → transcription → classification → action).\n",
    "\n",
    "- Human-in-the-Loop: The decision node for low-confidence samples ensures ongoing human supervision, which is excellent for quality control.\n",
    "\n",
    "- Model Maintenance: Logging results and updating a fingerprint database is a clever way to short-circuit repeated work on identical or near-identical audio.\n",
    "\n",
    "- Scalability: As call volumes grow, you might consider containerized deployments (e.g., Docker/Kubernetes), GPU acceleration, and automated scaling strategies.\n",
    "\n",
    "- Legal & Ethical Considerations: Always keep data privacy and consent in mind when dealing with call recordings.\n",
    "\n",
    "**Conclusion**\n",
    "Your plan is solid. It follows a well-recognized pattern for building ML systems in production: gather data, label it, train, deploy, monitor confidence, and improve via active learning. The references above should give you a solid foundation for each section of the pipeline. Good luck with your project, and don’t hesitate to iterate on the process as you learn more from real-world usage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e76c5a8-ee25-4ccf-a834-a705e4574b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # file_path = \"spam_ham_detection_sequence_diagram.mmd\"\n",
    "    # sequence_diagram = read_mmd_file(file_path=file_path)\n",
    "\n",
    "    # display_mermaid_diagram(diagram=sequence_diagram)\n",
    "\n",
    "    file_path = \"spam_ham_detection_flowchart_diagram.mmd\"\n",
    "    sequence_diagram = read_mmd_file(file_path=file_path)\n",
    "\n",
    "    display_mermaid_diagram(diagram=sequence_diagram)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "display_documentation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
