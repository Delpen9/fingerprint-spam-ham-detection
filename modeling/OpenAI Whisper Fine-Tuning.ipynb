{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03f1c9a8-7fc8-4134-83de-f356bb352268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch torchaudio transformers datasets librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9e7d99c-1d33-49e3-88ef-2a672a99ffce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperConfig\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "411b2e8c-7f68-4744-bedb-45db5a5c28f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_whisper_model(\n",
    "    whisper_model_name = \"openai/whisper-small\",\n",
    ") -> tuple[WhisperForConditionalGeneration, WhisperProcessor]:\n",
    "    # Load pre-trained Whisper model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name)\n",
    "    processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "\n",
    "    # Freeze the base Whisper model to retain pre-trained knowledge\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return (model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc02dd45-5dd5-4d14-88ab-4096fc97c9f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class WhisperWithClassification(nn.Module):\n",
    "    def __init__(self, whisper_model, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.whisper = whisper_model\n",
    "        self.fc = nn.Linear(whisper_model.config.d_model, num_classes)  # Classification head\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        # Pass through Whisper encoder\n",
    "        encoder_outputs = self.whisper.model.encoder(input_features).last_hidden_state\n",
    "\n",
    "        # Take the mean of encoder hidden states (global average pooling)\n",
    "        pooled_output = encoder_outputs.mean(dim=1)\n",
    "\n",
    "        # Pass through classification head\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe4629b7-6590-4f18-b425-210beda352eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, processor):\n",
    "    audio, sr = librosa.load(file_path, sr=16000)  # Whisper expects 16kHz\n",
    "    input_features = processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_features\n",
    "    return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1e16dd-3e7d-4da9-af16-39c1269245bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            audio_inputs, labels = batch  # Load batch data\n",
    "            \n",
    "            # Move to GPU\n",
    "            audio_inputs, labels = audio_inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(audio_inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4a3f43f-0927-419e-a7bd-971cb420fb17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, audio_path, processor, device):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_features = preprocess_audio(audio_path, processor).to(device)\n",
    "        logits = model(input_features)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        return \"Spam\" if prediction == 1 else \"Ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "436e1f1f-d78c-44b5-bd9c-814bf7b792c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, processor = load_whisper_model(whisper_model_name=\"openai/whisper-small\")\n",
    "\n",
    "    # Wrap Whisper model with classification head\n",
    "    num_classes = 2  # Spam vs Ham\n",
    "    model_with_classifier = WhisperWithClassification(model, num_classes)\n",
    "\n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_with_classifier.to(device)\n",
    "\n",
    "    # Example audio file path\n",
    "    audio_path = \"example.wav\"\n",
    "    input_features = preprocess_audio(audio_path, processor)\n",
    "\n",
    "    # Move input to the correct device\n",
    "    input_features = input_features.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_with_classifier.parameters(), lr=1e-4)\n",
    "\n",
    "    ############################\n",
    "    ## Model Training\n",
    "    ############################\n",
    "    dataloader = DataLoader(None, batch_size=8, shuffle=True)\n",
    "    train(model_with_classifier, dataloader, criterion, optimizer, device)\n",
    "\n",
    "    ############################\n",
    "    ## Inference\n",
    "    ############################\n",
    "    # Test on new audio\n",
    "    test_audio = \"new_call.wav\"\n",
    "    print(f\"Prediction: {predict(model_with_classifier, test_audio, processor, device)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "OpenAI Whisper Fine-Tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
